## 第4章 决策树

### 4.1 基本流程

​		一颗决策树包括：

> 根节点：包含样本全集；
>
> 叶节点：对应于决策结果；
>
> 其他节点：对应一个属性测试，每个节点包含的样本集合根据属性测试的结果划分到子节点里；

​		决策树的学习目的是训练出强泛化能力、处理未见示例能力强的决策树，基本的流程遵循简单直白的“分而治之”。

​		导致递归返回的三种情况：

* 当前结点包含的所有样本==属于同一类别==，无需划分
  * 同类不同值；
* 当前==属性集==为空或是所有样本在所有属性上取值相同，无法划分；
  * 无属性测试或者属性测试结果一致；
  * <u>将该结点设为叶结点，并将该结点所含样本最多的类别</u>；
* 当前结点包含的样本集合为空，不能划分；
  * 没有样本；
  * <u>将该结点设为叶结点，并将该结点的父结点所含样本最多的类别</u>；

### 4.2 划分选择



![image-20210721141843277](https://img-blog.csdnimg.cn/img_convert/eec8709912191d40ecc43d5986defdfc.png#pic_center)

* 决策树关键：如何选择最优划分属性
  * 结点“纯度”越来越高

三种度量样本集合纯度的指标：

* 信息增益

  信息熵：假定当前样本集合$D$中第$k$类样本所占比例为$p_{k}(k=1,2,...,|\mathbf{y}|)$，则$D$的信息熵定义为
  $$
  Ent(D)=-\sum_{k=1}^{\mathbf{|y|}}p_{k}log_2p_{k}\tag{4.1}
  $$
  $Ent(D)$的值越小，$D$的纯度越高。

  ​		假定离散属性$a$有$V$个可能取值，若使用$a$对$D$进行划分，则会产生$V$个分支节点，其中第$v$个分支结点包含了$D$中所有在属性$a^{v}$的样本，记为$D^{v}$。计算出信息熵，再考虑不同分支结点所包含的样本数不同，给分支结点赋予权重$\frac{D^{v}}{D}$，即样本数越多的分支结点的影响越大，于是可以计算出用属性$a$对样本集$D$进行划分所获得的“信息增益”
  $$
  Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{D^{v}}{D}Ent(D^{v})\tag{4.2}
  $$
  ​		信息增益越大，以为着使用属性$a$来进行划分所获得的“纯度提升”越大。因此，可以通过
  $$
  a_{*}=argmax_{a\in A}Gain(D,a)
  $$
  来选择最优划分属性。

* 增益率

  Why not 信息增益？

  > 如果考虑“编号”作为属性，会发现其信息熵远远大于其他指标，这是因为此属性的取值与样本一一对应，纯度最大但不具备泛化能力；
  >
  > -->信息增益对于属性值较多的属性有所偏好，所以考虑使用增益率

$$
Gain\_ ratio(D,a)=\frac{Gain(D,a)}{IV(a)}\tag{4.3}
$$

其中
$$
IV(a)=-\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}log_2 \frac{|D^{v}|}{|D|}\tag{4.4}
$$
称为$a$的==“固有值”==属性，属性的可能取值越多（$V$越大），固有值通常会越大。

​		需要注意的是，增益率准则对可取值数目较少的属性有所偏好，因此C4.5算法并不是直接选取增益率最大的候选划分属性，而是使用启发式算法：先从候选划分属性中找出信息增益高于平均水平的属性，再选取增益率最高的。

* 基尼指数

  基尼值
  $$
  \begin{aligned}
  Gini(D)&=\sum_{k=1}^{|\mathbf{y}|}\sum_{k'\neq k}p_{k}p_{k'}\\
  &=1-\sum_{k=1}^{|\mathbf{y}|}p_{k}^{2}\\
  \end{aligned}\tag{4.5}
  $$

​		直观来说，$Gini(D)$反映数据集$D$中随机抽取两个样本，其类别标记不一致的概率。因此，基尼值越小，数据集纯度越高。

​		属性$a$的基尼指数定义为
$$
Gini_index(D,a)=\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}Gini(D^{v})\tag{4.6}
$$
​		使得划分后基尼指数最小的属性作为最优划分属性。

### 4.3 剪枝处理

​		剪枝是决策树算法针对“过拟合”的主要手段。

* 预剪枝
  * 在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;
  * 使得决策树很多分支都没有“展开”，降低了过拟合风险，还显著减少了决策树的训练时间开销和测试时间开销；
  * 存在欠拟合的风险；
* 后剪枝
  * 先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.
  * 自下而上，欠拟合风险小，泛化能力更优；
  * 时间开销更大；

### 4.4 连续与缺失值

#### 连续值处理

​		由于连续属性的可取数目不再有限，使用连续属性离散化技术，如二分法（C4.5的机制）

​		给定样本集$D$和连续属性，假定$a$在$D$上出现了$n$个不同的取值，将这些值从小到大进行排序，记为$\{a^{1},a{2},...,a^{n}\}$。基于划分点$t$将样本集分为$D_{t}^{-}和D_{t}^{+}$

​		对于相邻的属性取值$a^{i}$和$a^{i+1}$来说，$t$在区间$[a^{i},a^{i+1})$中取任意值所产生的划分结果相同。

​		因此，我们考虑包含n-1的集合：
$$
T_{a}=\{\frac{a^{i}+a^{i+1}}{2}|1\leq i\leq n-1 \}\tag{4.7}
$$
将中位点作为候选划分点。作为候选划分点，我们就可以像离散属性值一样来考察这些划分点。
$$
\begin{aligned}
Gain(D,a)&=max_{t\in T_{a}}Gain(D,a,t)\\
&=max_{t\in T_{a}}Ent(D)-\sum_{\lambda \in \{-,+\}}\frac{|D_{t}^{\lambda}|}{|D|}Ent(D_{t}^{\lambda})
\end{aligned}\tag{4.8}
$$

#### 缺失值处理

两个step：

* 如何在属性值缺失的情况下进行划分属性的选择？
* 给定划分属性，若样本上该属性值缺失，如何对样本进行划分？
